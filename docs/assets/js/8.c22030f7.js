(window.webpackJsonp=window.webpackJsonp||[]).push([[8],{398:function(t,e,n){"use strict";n.r(e);var s=n(54),a=Object(s.a)({},(function(){var t=this,e=t.$createElement,n=t._self._c||e;return n("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[n("h1",{attrs:{id:"high-performance-computing"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#high-performance-computing"}},[t._v("#")]),t._v(" High Performance Computing")]),t._v(" "),n("p",{attrs:{align:"center"}},[n("img",{attrs:{src:"/hpc_logo.png",width:"300em"}})]),t._v(" "),n("p",[n("strong",[t._v("Damavand")]),t._v(" is written to support computations on conventional laptops or HPC architectures.\nHere, we describe the distributed implementation and provide with some scripts to start with.")]),t._v(" "),n("h2",{attrs:{id:"distributed-cpu"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#distributed-cpu"}},[t._v("#")]),t._v(" Distributed CPU")]),t._v(" "),n("p",[t._v("First, let us pass through the distributed CPU implementation. Consider that you have access to a supercomputer, or an\narchitecture that supports multiple nodes. One advantage of this is that quantum circuit simulations are "),n("strong",[t._v("memory\nbound")]),t._v(". This means that the computation bottleneck does not reside on the operations - that are pretty simple - but\nrather on how to store the excessive amount of memory required to simulate a quantum state.")]),t._v(" "),n("p",[t._v("As shown in the illustration below, condier that the state vector can be splitted into a fixed number N. It is then\nstraightforward to assign each chunk of the state vector to a different node.")]),t._v(" "),n("p",[n("img",{attrs:{src:"/damavand_cpu_distributed.png",alt:"Distributed GPU"}})]),t._v(" "),n("p",[t._v("Each node is built with a certain number of processors, thus allowing to run multithreaded experiments. However,\nreferring to the multithreading implementation presented in Guide, we can infer that some communications will be\nrequired between distant nodes to share a part of the state vector. Indeed, the stride between two amplitudes is\nnecessarily of length 2^k, where k is the target qubit on which one wants to perform some operation.")]),t._v(" "),n("p",[t._v("Damavand implements an MPI scheme in order to share this information across multiple nodes.")]),t._v(" "),n("h2",{attrs:{id:"distributed-gpu"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#distributed-gpu"}},[t._v("#")]),t._v(" Distributed GPU")]),t._v(" "),n("p",[t._v("Another possibility is to leverage architectures - mainly built to support Artificial Intelligence applications -\ncomposed of multiple nodes, each containing multiple GPUs. As seen in the Guide section, the single GPU implementation\nis almost straightforward, and although some optimizations could be further thought of, it is reasonable to consider the\nmulti GPU implementation. We focus on "),n("a",{attrs:{href:"https://developer.nvidia.com/cuda-zone",target:"_blank",rel:"noopener noreferrer"}},[t._v("CUDA"),n("OutboundLink")],1),t._v(" capable devices, thus restraining\nthe developments to "),n("strong",[t._v("Nvidia")]),t._v(" hardware.")]),t._v(" "),n("p",[t._v("In the case of a supercomputer, the GPUs on the same node will often allow direct inter device communications through\nhigh thoughput NVLinks.")]),t._v(" "),n("p",[n("img",{attrs:{src:"/damavand_gpu_distributed.png",alt:"Distributed GPU"}})]),t._v(" "),n("p",[t._v("The illustration above shows that memory can be loaded on from the CPUs to the GPUs. We choose to treat inter-node\ncommunications with MPI ("),n("a",{attrs:{href:"https://www.open-mpi.org/",target:"_blank",rel:"noopener noreferrer"}},[t._v("OpenMPI"),n("OutboundLink")],1),t._v(" or "),n("a",{attrs:{href:"https://www.mpich.org/",target:"_blank",rel:"noopener noreferrer"}},[t._v("MPICH"),n("OutboundLink")],1),t._v(").")]),t._v(" "),n("h2",{attrs:{id:"slurm-scripts"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#slurm-scripts"}},[t._v("#")]),t._v(" Slurm scripts")]),t._v(" "),n("div",{staticClass:"language-bash extra-class"},[n("pre",{pre:!0,attrs:{class:"language-bash"}},[n("code",[n("span",{pre:!0,attrs:{class:"token shebang important"}},[t._v("#!/bin/bash")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#SBATCH --job-name=two_nodes")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#SBATCH --qos=qos_cpu-dev")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#SBATCH --ntasks=2")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#SBATCH --ntasks-per-node=1")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#SBATCH --cpus-per-task=40")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#SBATCH --output=two_nodes.listing")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#SBATCH --time=5:00")]),t._v("\n\nmodule purge\n\nmodule load openmpi/3.1.4\nmodule load cuda/10.2\n\nsrun python3 two_nodes.py\n")])])]),n("h2",{attrs:{id:"experimentations-on-jean-zay"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#experimentations-on-jean-zay"}},[t._v("#")]),t._v(" Experimentations on Jean-Zay")]),t._v(" "),n("p",{attrs:{align:"center"}},[n("img",{attrs:{src:"/jean-zay.jpg",width:"600em"}})])])}),[],!1,null,null,null);e.default=a.exports}}]);