<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>High Performance Computing</title>
    <meta name="generator" content="VuePress 1.8.3">
    
    <meta name="description" content="">
    
    <link rel="preload" href="/assets/css/0.styles.e4d8b86b.css" as="style"><link rel="preload" href="/assets/js/app.69676b56.js" as="script"><link rel="preload" href="/assets/js/2.85fb4fdd.js" as="script"><link rel="preload" href="/assets/js/8.c22030f7.js" as="script"><link rel="prefetch" href="/assets/js/10.2f026394.js"><link rel="prefetch" href="/assets/js/11.5393efdd.js"><link rel="prefetch" href="/assets/js/12.67031a6c.js"><link rel="prefetch" href="/assets/js/3.1ac3f9ad.js"><link rel="prefetch" href="/assets/js/4.948b6615.js"><link rel="prefetch" href="/assets/js/5.686a82d5.js"><link rel="prefetch" href="/assets/js/6.3f4ffd09.js"><link rel="prefetch" href="/assets/js/7.ed9f93d3.js"><link rel="prefetch" href="/assets/js/9.042a3fc4.js">
    <link rel="stylesheet" href="/assets/css/0.styles.e4d8b86b.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container no-sidebar"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/" class="home-link router-link-active"></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <!----></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><!---->  <!----> </aside> <main class="page"> <div class="theme-default-content content__default"><h1 id="high-performance-computing"><a href="#high-performance-computing" class="header-anchor">#</a> High Performance Computing</h1> <p align="center"><img src="/hpc_logo.png" width="300em"></p> <p><strong>Damavand</strong> is written to support computations on conventional laptops or HPC architectures.
Here, we describe the distributed implementation and provide with some scripts to start with.</p> <h2 id="distributed-cpu"><a href="#distributed-cpu" class="header-anchor">#</a> Distributed CPU</h2> <p>First, let us pass through the distributed CPU implementation. Consider that you have access to a supercomputer, or an
architecture that supports multiple nodes. One advantage of this is that quantum circuit simulations are <strong>memory
bound</strong>. This means that the computation bottleneck does not reside on the operations - that are pretty simple - but
rather on how to store the excessive amount of memory required to simulate a quantum state.</p> <p>As shown in the illustration below, condier that the state vector can be splitted into a fixed number N. It is then
straightforward to assign each chunk of the state vector to a different node.</p> <p><img src="/damavand_cpu_distributed.png" alt="Distributed GPU"></p> <p>Each node is built with a certain number of processors, thus allowing to run multithreaded experiments. However,
referring to the multithreading implementation presented in Guide, we can infer that some communications will be
required between distant nodes to share a part of the state vector. Indeed, the stride between two amplitudes is
necessarily of length 2^k, where k is the target qubit on which one wants to perform some operation.</p> <p>Damavand implements an MPI scheme in order to share this information across multiple nodes.</p> <h2 id="distributed-gpu"><a href="#distributed-gpu" class="header-anchor">#</a> Distributed GPU</h2> <p>Another possibility is to leverage architectures - mainly built to support Artificial Intelligence applications -
composed of multiple nodes, each containing multiple GPUs. As seen in the Guide section, the single GPU implementation
is almost straightforward, and although some optimizations could be further thought of, it is reasonable to consider the
multi GPU implementation. We focus on <a href="https://developer.nvidia.com/cuda-zone" target="_blank" rel="noopener noreferrer">CUDA<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> capable devices, thus restraining
the developments to <strong>Nvidia</strong> hardware.</p> <p>In the case of a supercomputer, the GPUs on the same node will often allow direct inter device communications through
high thoughput NVLinks.</p> <p><img src="/damavand_gpu_distributed.png" alt="Distributed GPU"></p> <p>The illustration above shows that memory can be loaded on from the CPUs to the GPUs. We choose to treat inter-node
communications with MPI (<a href="https://www.open-mpi.org/" target="_blank" rel="noopener noreferrer">OpenMPI<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> or <a href="https://www.mpich.org/" target="_blank" rel="noopener noreferrer">MPICH<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>).</p> <h2 id="slurm-scripts"><a href="#slurm-scripts" class="header-anchor">#</a> Slurm scripts</h2> <div class="language-bash extra-class"><pre class="language-bash"><code><span class="token shebang important">#!/bin/bash</span>

<span class="token comment">#SBATCH --job-name=two_nodes</span>
<span class="token comment">#SBATCH --qos=qos_cpu-dev</span>
<span class="token comment">#SBATCH --ntasks=2</span>
<span class="token comment">#SBATCH --ntasks-per-node=1</span>
<span class="token comment">#SBATCH --cpus-per-task=40</span>
<span class="token comment">#SBATCH --output=two_nodes.listing</span>
<span class="token comment">#SBATCH --time=5:00</span>

module purge

module load openmpi/3.1.4
module load cuda/10.2

srun python3 two_nodes.py
</code></pre></div><h2 id="experimentations-on-jean-zay"><a href="#experimentations-on-jean-zay" class="header-anchor">#</a> Experimentations on Jean-Zay</h2> <p align="center"><img src="/jean-zay.jpg" width="600em"></p></div> <footer class="page-edit"><!----> <!----></footer> <!----> </main></div><div class="global-ui"></div></div>
    <script src="/assets/js/app.69676b56.js" defer></script><script src="/assets/js/2.85fb4fdd.js" defer></script><script src="/assets/js/8.c22030f7.js" defer></script>
  </body>
</html>
